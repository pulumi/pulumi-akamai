// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs } from "./types";
import * as utilities from "./utilities";

/**
 * ## Import
 *
 * Basic usagehcl resource "akamai_datastream" "example" {
 *
 * # (resource arguments)
 *
 *  } You can import your Akamai DataStream configuration using a stream version ID. For example
 *
 * ```sh
 *  $ pulumi import akamai:index/datastream:Datastream example 1234
 * ```
 *
 *  ~> **IMPORTANT:** For security reasons, this command doesn't import any secrets you specify for your connector. To make sure the state file includes complete data, use this resource to manually add the arguments marked **Secret** above.
 */
export class Datastream extends pulumi.CustomResource {
    /**
     * Get an existing Datastream resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: DatastreamState, opts?: pulumi.CustomResourceOptions): Datastream {
        return new Datastream(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'akamai:index/datastream:Datastream';

    /**
     * Returns true if the given object is an instance of Datastream.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is Datastream {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Datastream.__pulumiType;
    }

    /**
     * - (Required) Whether you want to start activating the stream when applying the resource. Either `true` for activating the stream upon sending the request or `false` for leaving the stream inactive after the request.
     */
    public readonly active!: pulumi.Output<boolean>;
    /**
     * Specify details about the Azure Storage connector configuration in a data stream. Note that currently DataStream supports only streaming data to [block objects](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs). The argument includes these sub-arguments:
     */
    public readonly azureConnector!: pulumi.Output<outputs.DatastreamAzureConnector | undefined>;
    /**
     * - (Required) Provides information about the log line configuration, log file format, names of log files sent, and file delivery. The argument includes these sub-arguments:
     */
    public readonly config!: pulumi.Output<outputs.DatastreamConfig>;
    /**
     * - (Required) Identifies the contract that has access to the product.
     */
    public readonly contractId!: pulumi.Output<string>;
    /**
     * The username who created the stream
     */
    public /*out*/ readonly createdBy!: pulumi.Output<string>;
    /**
     * The date and time when the stream was created
     */
    public /*out*/ readonly createdDate!: pulumi.Output<string>;
    /**
     * Specify details about the Datadog connector in a stream, including:
     */
    public readonly datadogConnector!: pulumi.Output<outputs.DatastreamDatadogConnector | undefined>;
    /**
     * - (Required)	Identifiers of the data set fields within the template that you want to receive in logs. The order of the identifiers define how the value for these fields appears in the log lines. See [Data set parameters](https://techdocs.akamai.com/datastream2/reference/data-set-parameters-1).
     */
    public readonly datasetFieldsIds!: pulumi.Output<number[]>;
    /**
     * - (Optional) A list of email addresses you want to notify about activations and deactivations of the stream.
     */
    public readonly emailIds!: pulumi.Output<string[] | undefined>;
    /**
     * Specify details about the Google Cloud Storage connector you can use in a stream. When validating this connector, DataStream uses the private access key to create an `Akamai_access_verification_<timestamp>.txt` object file in your GCS bucket. You can only see this file if the validation process is successful, and you have access to the Google Cloud Storage bucket where you are trying to send logs. The argument includes these sub-arguments:
     */
    public readonly gcsConnector!: pulumi.Output<outputs.DatastreamGcsConnector | undefined>;
    /**
     * - (Required) Identifies the group that has access to the product and this stream configuration.
     */
    public readonly groupId!: pulumi.Output<string>;
    /**
     * The name of the user group for which the stream was created
     */
    public /*out*/ readonly groupName!: pulumi.Output<string>;
    /**
     * Specify details about the custom HTTPS endpoint you can use as a connector for a stream, including:
     */
    public readonly httpsConnector!: pulumi.Output<outputs.DatastreamHttpsConnector | undefined>;
    /**
     * The username who modified the stream
     */
    public /*out*/ readonly modifiedBy!: pulumi.Output<string>;
    /**
     * The date and time when the stream was modified
     */
    public /*out*/ readonly modifiedDate!: pulumi.Output<string>;
    /**
     * Specify details about the Oracle Cloud Storage connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and tries to save an `Akamai_access_verification_<timestamp>.txt` file in your Oracle Cloud Storage folder. You can only see this file if the validation process is successful, and you have access to the Oracle Cloud Storage bucket and folder that you’re trying to send logs to.
     */
    public readonly oracleConnector!: pulumi.Output<outputs.DatastreamOracleConnector | undefined>;
    /**
     * The configuration in JSON format that can be copy-pasted into PAPI configuration to enable datastream behavior
     */
    public /*out*/ readonly papiJson!: pulumi.Output<string>;
    /**
     * The ID of the product for which the stream was created
     */
    public /*out*/ readonly productId!: pulumi.Output<string>;
    /**
     * The name of the product for which the stream was created
     */
    public /*out*/ readonly productName!: pulumi.Output<string>;
    /**
     * - (Required) Identifies the properties that you want to monitor in the stream. Note that a stream can only log data for active properties.
     */
    public readonly propertyIds!: pulumi.Output<string[]>;
    /**
     * - (Optional) Specify details about the Amazon S3 connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and saves an `akamai_write_test_2147483647.txt` file in your Amazon S3 folder. You can only see this file if validation succeeds, and you have access to the Amazon S3 bucket and folder that you’re trying to send logs to. The argument includes these sub-arguments:
     */
    public readonly s3Connector!: pulumi.Output<outputs.DatastreamS3Connector | undefined>;
    /**
     * Specify details about the Splunk connector in your stream. Note that currently DataStream supports only endpoint URLs ending with `collector/raw`. The argument includes these sub-arguments:
     */
    public readonly splunkConnector!: pulumi.Output<outputs.DatastreamSplunkConnector | undefined>;
    /**
     * - (Required) The name of the stream.
     */
    public readonly streamName!: pulumi.Output<string>;
    /**
     * - (Required) The type of stream that you want to create. Currently, `RAW_LOGS` is the only possible stream type.
     */
    public readonly streamType!: pulumi.Output<string>;
    /**
     * Identifies the configuration version of the stream
     */
    public /*out*/ readonly streamVersionId!: pulumi.Output<number>;
    /**
     * Specify details about the Sumo Logic connector in a stream, including:
     */
    public readonly sumologicConnector!: pulumi.Output<outputs.DatastreamSumologicConnector | undefined>;
    /**
     * - (Required) The name of the data set template available for the product that you want to use in the stream. Currently, `EDGE_LOGS` is the only data set template available.
     */
    public readonly templateName!: pulumi.Output<string>;

    /**
     * Create a Datastream resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: DatastreamArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: DatastreamArgs | DatastreamState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as DatastreamState | undefined;
            resourceInputs["active"] = state ? state.active : undefined;
            resourceInputs["azureConnector"] = state ? state.azureConnector : undefined;
            resourceInputs["config"] = state ? state.config : undefined;
            resourceInputs["contractId"] = state ? state.contractId : undefined;
            resourceInputs["createdBy"] = state ? state.createdBy : undefined;
            resourceInputs["createdDate"] = state ? state.createdDate : undefined;
            resourceInputs["datadogConnector"] = state ? state.datadogConnector : undefined;
            resourceInputs["datasetFieldsIds"] = state ? state.datasetFieldsIds : undefined;
            resourceInputs["emailIds"] = state ? state.emailIds : undefined;
            resourceInputs["gcsConnector"] = state ? state.gcsConnector : undefined;
            resourceInputs["groupId"] = state ? state.groupId : undefined;
            resourceInputs["groupName"] = state ? state.groupName : undefined;
            resourceInputs["httpsConnector"] = state ? state.httpsConnector : undefined;
            resourceInputs["modifiedBy"] = state ? state.modifiedBy : undefined;
            resourceInputs["modifiedDate"] = state ? state.modifiedDate : undefined;
            resourceInputs["oracleConnector"] = state ? state.oracleConnector : undefined;
            resourceInputs["papiJson"] = state ? state.papiJson : undefined;
            resourceInputs["productId"] = state ? state.productId : undefined;
            resourceInputs["productName"] = state ? state.productName : undefined;
            resourceInputs["propertyIds"] = state ? state.propertyIds : undefined;
            resourceInputs["s3Connector"] = state ? state.s3Connector : undefined;
            resourceInputs["splunkConnector"] = state ? state.splunkConnector : undefined;
            resourceInputs["streamName"] = state ? state.streamName : undefined;
            resourceInputs["streamType"] = state ? state.streamType : undefined;
            resourceInputs["streamVersionId"] = state ? state.streamVersionId : undefined;
            resourceInputs["sumologicConnector"] = state ? state.sumologicConnector : undefined;
            resourceInputs["templateName"] = state ? state.templateName : undefined;
        } else {
            const args = argsOrState as DatastreamArgs | undefined;
            if ((!args || args.active === undefined) && !opts.urn) {
                throw new Error("Missing required property 'active'");
            }
            if ((!args || args.config === undefined) && !opts.urn) {
                throw new Error("Missing required property 'config'");
            }
            if ((!args || args.contractId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'contractId'");
            }
            if ((!args || args.datasetFieldsIds === undefined) && !opts.urn) {
                throw new Error("Missing required property 'datasetFieldsIds'");
            }
            if ((!args || args.groupId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'groupId'");
            }
            if ((!args || args.propertyIds === undefined) && !opts.urn) {
                throw new Error("Missing required property 'propertyIds'");
            }
            if ((!args || args.streamName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'streamName'");
            }
            if ((!args || args.streamType === undefined) && !opts.urn) {
                throw new Error("Missing required property 'streamType'");
            }
            if ((!args || args.templateName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'templateName'");
            }
            resourceInputs["active"] = args ? args.active : undefined;
            resourceInputs["azureConnector"] = args ? args.azureConnector : undefined;
            resourceInputs["config"] = args ? args.config : undefined;
            resourceInputs["contractId"] = args ? args.contractId : undefined;
            resourceInputs["datadogConnector"] = args ? args.datadogConnector : undefined;
            resourceInputs["datasetFieldsIds"] = args ? args.datasetFieldsIds : undefined;
            resourceInputs["emailIds"] = args ? args.emailIds : undefined;
            resourceInputs["gcsConnector"] = args ? args.gcsConnector : undefined;
            resourceInputs["groupId"] = args ? args.groupId : undefined;
            resourceInputs["httpsConnector"] = args ? args.httpsConnector : undefined;
            resourceInputs["oracleConnector"] = args ? args.oracleConnector : undefined;
            resourceInputs["propertyIds"] = args ? args.propertyIds : undefined;
            resourceInputs["s3Connector"] = args ? args.s3Connector : undefined;
            resourceInputs["splunkConnector"] = args ? args.splunkConnector : undefined;
            resourceInputs["streamName"] = args ? args.streamName : undefined;
            resourceInputs["streamType"] = args ? args.streamType : undefined;
            resourceInputs["sumologicConnector"] = args ? args.sumologicConnector : undefined;
            resourceInputs["templateName"] = args ? args.templateName : undefined;
            resourceInputs["createdBy"] = undefined /*out*/;
            resourceInputs["createdDate"] = undefined /*out*/;
            resourceInputs["groupName"] = undefined /*out*/;
            resourceInputs["modifiedBy"] = undefined /*out*/;
            resourceInputs["modifiedDate"] = undefined /*out*/;
            resourceInputs["papiJson"] = undefined /*out*/;
            resourceInputs["productId"] = undefined /*out*/;
            resourceInputs["productName"] = undefined /*out*/;
            resourceInputs["streamVersionId"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Datastream.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering Datastream resources.
 */
export interface DatastreamState {
    /**
     * - (Required) Whether you want to start activating the stream when applying the resource. Either `true` for activating the stream upon sending the request or `false` for leaving the stream inactive after the request.
     */
    active?: pulumi.Input<boolean>;
    /**
     * Specify details about the Azure Storage connector configuration in a data stream. Note that currently DataStream supports only streaming data to [block objects](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs). The argument includes these sub-arguments:
     */
    azureConnector?: pulumi.Input<inputs.DatastreamAzureConnector>;
    /**
     * - (Required) Provides information about the log line configuration, log file format, names of log files sent, and file delivery. The argument includes these sub-arguments:
     */
    config?: pulumi.Input<inputs.DatastreamConfig>;
    /**
     * - (Required) Identifies the contract that has access to the product.
     */
    contractId?: pulumi.Input<string>;
    /**
     * The username who created the stream
     */
    createdBy?: pulumi.Input<string>;
    /**
     * The date and time when the stream was created
     */
    createdDate?: pulumi.Input<string>;
    /**
     * Specify details about the Datadog connector in a stream, including:
     */
    datadogConnector?: pulumi.Input<inputs.DatastreamDatadogConnector>;
    /**
     * - (Required)	Identifiers of the data set fields within the template that you want to receive in logs. The order of the identifiers define how the value for these fields appears in the log lines. See [Data set parameters](https://techdocs.akamai.com/datastream2/reference/data-set-parameters-1).
     */
    datasetFieldsIds?: pulumi.Input<pulumi.Input<number>[]>;
    /**
     * - (Optional) A list of email addresses you want to notify about activations and deactivations of the stream.
     */
    emailIds?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specify details about the Google Cloud Storage connector you can use in a stream. When validating this connector, DataStream uses the private access key to create an `Akamai_access_verification_<timestamp>.txt` object file in your GCS bucket. You can only see this file if the validation process is successful, and you have access to the Google Cloud Storage bucket where you are trying to send logs. The argument includes these sub-arguments:
     */
    gcsConnector?: pulumi.Input<inputs.DatastreamGcsConnector>;
    /**
     * - (Required) Identifies the group that has access to the product and this stream configuration.
     */
    groupId?: pulumi.Input<string>;
    /**
     * The name of the user group for which the stream was created
     */
    groupName?: pulumi.Input<string>;
    /**
     * Specify details about the custom HTTPS endpoint you can use as a connector for a stream, including:
     */
    httpsConnector?: pulumi.Input<inputs.DatastreamHttpsConnector>;
    /**
     * The username who modified the stream
     */
    modifiedBy?: pulumi.Input<string>;
    /**
     * The date and time when the stream was modified
     */
    modifiedDate?: pulumi.Input<string>;
    /**
     * Specify details about the Oracle Cloud Storage connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and tries to save an `Akamai_access_verification_<timestamp>.txt` file in your Oracle Cloud Storage folder. You can only see this file if the validation process is successful, and you have access to the Oracle Cloud Storage bucket and folder that you’re trying to send logs to.
     */
    oracleConnector?: pulumi.Input<inputs.DatastreamOracleConnector>;
    /**
     * The configuration in JSON format that can be copy-pasted into PAPI configuration to enable datastream behavior
     */
    papiJson?: pulumi.Input<string>;
    /**
     * The ID of the product for which the stream was created
     */
    productId?: pulumi.Input<string>;
    /**
     * The name of the product for which the stream was created
     */
    productName?: pulumi.Input<string>;
    /**
     * - (Required) Identifies the properties that you want to monitor in the stream. Note that a stream can only log data for active properties.
     */
    propertyIds?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * - (Optional) Specify details about the Amazon S3 connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and saves an `akamai_write_test_2147483647.txt` file in your Amazon S3 folder. You can only see this file if validation succeeds, and you have access to the Amazon S3 bucket and folder that you’re trying to send logs to. The argument includes these sub-arguments:
     */
    s3Connector?: pulumi.Input<inputs.DatastreamS3Connector>;
    /**
     * Specify details about the Splunk connector in your stream. Note that currently DataStream supports only endpoint URLs ending with `collector/raw`. The argument includes these sub-arguments:
     */
    splunkConnector?: pulumi.Input<inputs.DatastreamSplunkConnector>;
    /**
     * - (Required) The name of the stream.
     */
    streamName?: pulumi.Input<string>;
    /**
     * - (Required) The type of stream that you want to create. Currently, `RAW_LOGS` is the only possible stream type.
     */
    streamType?: pulumi.Input<string>;
    /**
     * Identifies the configuration version of the stream
     */
    streamVersionId?: pulumi.Input<number>;
    /**
     * Specify details about the Sumo Logic connector in a stream, including:
     */
    sumologicConnector?: pulumi.Input<inputs.DatastreamSumologicConnector>;
    /**
     * - (Required) The name of the data set template available for the product that you want to use in the stream. Currently, `EDGE_LOGS` is the only data set template available.
     */
    templateName?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a Datastream resource.
 */
export interface DatastreamArgs {
    /**
     * - (Required) Whether you want to start activating the stream when applying the resource. Either `true` for activating the stream upon sending the request or `false` for leaving the stream inactive after the request.
     */
    active: pulumi.Input<boolean>;
    /**
     * Specify details about the Azure Storage connector configuration in a data stream. Note that currently DataStream supports only streaming data to [block objects](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs). The argument includes these sub-arguments:
     */
    azureConnector?: pulumi.Input<inputs.DatastreamAzureConnector>;
    /**
     * - (Required) Provides information about the log line configuration, log file format, names of log files sent, and file delivery. The argument includes these sub-arguments:
     */
    config: pulumi.Input<inputs.DatastreamConfig>;
    /**
     * - (Required) Identifies the contract that has access to the product.
     */
    contractId: pulumi.Input<string>;
    /**
     * Specify details about the Datadog connector in a stream, including:
     */
    datadogConnector?: pulumi.Input<inputs.DatastreamDatadogConnector>;
    /**
     * - (Required)	Identifiers of the data set fields within the template that you want to receive in logs. The order of the identifiers define how the value for these fields appears in the log lines. See [Data set parameters](https://techdocs.akamai.com/datastream2/reference/data-set-parameters-1).
     */
    datasetFieldsIds: pulumi.Input<pulumi.Input<number>[]>;
    /**
     * - (Optional) A list of email addresses you want to notify about activations and deactivations of the stream.
     */
    emailIds?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specify details about the Google Cloud Storage connector you can use in a stream. When validating this connector, DataStream uses the private access key to create an `Akamai_access_verification_<timestamp>.txt` object file in your GCS bucket. You can only see this file if the validation process is successful, and you have access to the Google Cloud Storage bucket where you are trying to send logs. The argument includes these sub-arguments:
     */
    gcsConnector?: pulumi.Input<inputs.DatastreamGcsConnector>;
    /**
     * - (Required) Identifies the group that has access to the product and this stream configuration.
     */
    groupId: pulumi.Input<string>;
    /**
     * Specify details about the custom HTTPS endpoint you can use as a connector for a stream, including:
     */
    httpsConnector?: pulumi.Input<inputs.DatastreamHttpsConnector>;
    /**
     * Specify details about the Oracle Cloud Storage connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and tries to save an `Akamai_access_verification_<timestamp>.txt` file in your Oracle Cloud Storage folder. You can only see this file if the validation process is successful, and you have access to the Oracle Cloud Storage bucket and folder that you’re trying to send logs to.
     */
    oracleConnector?: pulumi.Input<inputs.DatastreamOracleConnector>;
    /**
     * - (Required) Identifies the properties that you want to monitor in the stream. Note that a stream can only log data for active properties.
     */
    propertyIds: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * - (Optional) Specify details about the Amazon S3 connector in a stream. When validating this connector, DataStream uses the provided `accessKey` and `secretAccessKey` values and saves an `akamai_write_test_2147483647.txt` file in your Amazon S3 folder. You can only see this file if validation succeeds, and you have access to the Amazon S3 bucket and folder that you’re trying to send logs to. The argument includes these sub-arguments:
     */
    s3Connector?: pulumi.Input<inputs.DatastreamS3Connector>;
    /**
     * Specify details about the Splunk connector in your stream. Note that currently DataStream supports only endpoint URLs ending with `collector/raw`. The argument includes these sub-arguments:
     */
    splunkConnector?: pulumi.Input<inputs.DatastreamSplunkConnector>;
    /**
     * - (Required) The name of the stream.
     */
    streamName: pulumi.Input<string>;
    /**
     * - (Required) The type of stream that you want to create. Currently, `RAW_LOGS` is the only possible stream type.
     */
    streamType: pulumi.Input<string>;
    /**
     * Specify details about the Sumo Logic connector in a stream, including:
     */
    sumologicConnector?: pulumi.Input<inputs.DatastreamSumologicConnector>;
    /**
     * - (Required) The name of the data set template available for the product that you want to use in the stream. Currently, `EDGE_LOGS` is the only data set template available.
     */
    templateName: pulumi.Input<string>;
}
